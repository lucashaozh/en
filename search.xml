<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Ray Tracing Renderer (Course Project)</title>
      <link href="/en/2023/12/24/gamegraphics/noriproject/"/>
      <url>/en/2023/12/24/gamegraphics/noriproject/</url>
      
        <content type="html"><![CDATA[<h1 id="motivational-image">Motivational Image</h1><p>The scene that we want to render is a dimly lit, smoky bar, inspiredby the following two images. Rows of exquisite liquor bottles line theshelves, their labels veiled in the subtle play of light and shadow. Anashtray rests nearby, adorned with an unlit cigarette. A dim spotlightbathes the ashtray and the wine bottles. The scene is to capture theessence of the bar's mystique, inviting participants to let theirimagination run wild as they bring out the richness of the hiddenstories within the rendering. The following two images are ourmotivation images.</p><p><img src="https://s2.loli.net/2024/01/02/FHRUOCLqBpaJQgw.jpg" /> <imgsrc="https://s2.loli.net/2024/01/02/fexQVhryYm2gdnl.jpg" /></p><h1 id="final-images">Final Images</h1><p>Based on the motivation images, we rendered our final images.</p><p><img src="https://s2.loli.net/2024/01/02/SVn3gKIpMBth4ND.png" /></p><span id="more"></span><h1 id="features-implemented">Features implemented</h1><h2 id="images-as-texture">Images as Texture</h2><p><strong>Relevant Code</strong></p><ul><li><code>imagetexture.cpp</code></li></ul><h3 id="implementation-details">Implementation Details</h3><p>To load the texture from external files, I used<code>stb_image</code> to read the image data. I defined a<code>ImageData</code> class similar to <code>Bitmap</code> class, whichis drived from <code>Eigen::Array</code> to store the image data. Theread image data is probably in sRGB color space. Therefore, we providean option boolean field <code>raw</code> in the <code>img_texture</code>property to indicate whether use raw image color space or convert it tolinear color space with <code>toLinearRGB()</code> function. We alsoprovide two difference filter modes, <code>nearest</code> and<code>bilinear</code>.</p><h3 id="validation">Validation</h3><p>To validate the results, we used a texture from the website <ahref="https://www.artstation.com/blogs/zeeshannasir/peYz/reference-uv-map-grids">Reference:UV Map Grids</a> and the scenes in assignment for rendering. The resultslook right. We also compared our results with mitsuba. Since thecoordinate used in mitsuba is right-handed while nori's is left-handed,the up axis coordinate is inversed so that the scene representation isthe same. The results are quite similar, while the results of mitsubaseem to be more vivid than ours. I think this may due to the usage ofmipmap in mitsuba. The overall results are correct.</p><div class="twentytwenty-container" style="max-width:720px"><img src="plane-texture-nori.png" data-width="720"><img src="plane-texture-v06.png" data-width="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="floor-texture-nori-nearest.png" data-width="720"><img src="floor-texture-v06-nearest.png" data-width="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="mesh-texture-nori-bilinear.png" data-width="720"><img src="mesh-texture-v06-bilinear.png" data-width="720"></div><h2 id="normal-mapping">Normal Mapping</h2><p><strong>Relevant Code</strong></p><ul><li><code>mesh.h</code></li><li><code>mesh.cpp</code></li><li><code>shape.h</code></li><li><code>shape.cpp</code></li></ul><h3 id="implementation-details-1">Implementation Details</h3><p>Normal mapping requires us to update the mesh normal with the normalsprovided in the normal textures. Since the frame normal is calculated inthe <code>setHitInformation()</code> function in <code>Shape</code>, Idecided to read the normal map and update the normals here. I added afield of type <code>Texture&lt;Color3f&gt; m_normalMap</code>. Thenormal map can be loaded with the Image as Texture implemented before.After reading the texture evaled value, it has to be converted to range[-1, 1]. Currently, only <code>Mesh</code> supports computing shaingframe with normal mapping. In the <code>setHitInformation()</code>function in <code>Mesh</code>, the local normal read from the normalmapping needs to be converted to the world normal using the local framefirst. However, if the default frame setup is used, the result isdiscontinuous and is absolutely wrong. Therefore, I need to constructthe right local frame first. I followed the formula in <ahref="https://learnopengl.com/Advanced-Lighting/Normal-Mapping">learnopengl</a>.</p><p><span class="math display">\[\begin{bmatrix}T_{x}&amp;T_{y}&amp;T_{z}\\B_{x}&amp;B_{y}&amp;B_{z}\end{bmatrix}=\frac{1}{\Delta U_{1}\DeltaV_{2}-\Delta U_{2}\Delta V_{1}}\begin{bmatrix}\Delta V_{2}&amp;-\DeltaV_{1} \\ -\Delta U_{2} &amp; \Delta U_{1}\end{bmatrix}\begin{bmatrix}E_{1x}&amp;E_{1y}&amp;E_{1z}\\E_{2x}&amp;E_{2y}&amp;E_{2z}\end{bmatrix}\]</span></p><p>where <span class="math inline">\(E_{1}\)</span> and <spanclass="math inline">\(E_{2}\)</span> are two edges of the triangle, and<span class="math inline">\(\Delta U, \Delta V\)</span> are thecorresponding uv offsets of the edges. <spanclass="math inline">\(T\)</span> and <spanclass="math inline">\(B\)</span> is <code>dpdv</code> and<code>dpdu</code>. Then, we can construct the local frame correctly with<code>dpdv</code> and <code>dpdu</code>.</p><h3 id="validation-1">Validation</h3><p>To validate the results, the results of nori are compared to theresults of mitsuba.</p><div class="twentytwenty-container" style="max-width:720px"><img src="plane-normalmap-c-nori.png" data-width="720" data-height="720"><img src="plane-normalmap-c-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="plane-normalmap-t-nori.png" data-width="720" data-height="720"><img src="plane-normalmap-t-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="floor-normalmap-c-nori.png" data-width="720" data-height="720"><img src="floor-normalmap-c-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="floor-normalmap-t-nori.png" data-width="720" data-height="720"><img src="floor-normalmap-t-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="sphere-mesh-normalmap-c-nori.png" data-width="720" data-height="720"><img src="sphere-mesh-normalmap-c-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="sphere-mesh-normalmap-t-nori.png" data-width="720" data-height="720"><img src="sphere-mesh-normalmap-t-v06.png" data-width="720" data-height="720"></div><h2 id="emitter-spotlight-5pt">Emitter (Spotlight) (5pt)</h2><p><strong>Relevant Code</strong></p><ul><li><code>spotlight.cpp</code></li><li><code>path_mis.cpp</code></li><li><code>direct_mis.cpp</code></li></ul><h3 id="implementation-details-2">Implementation Details</h3><p>The implementation of the spotlight follows the mistuba designs. Ithas three properties, <code>intensity</code>, <code>maxAngle</code> and<code>beamAngle</code>. <code>intensity</code> is the max irridiance ofemitter at the center. All the ray emitted within the beamAngle has anirridiance of <code>intensity</code>. The emitted intensity of the raybegins to attenuate linearly between beamAngle and maxAngle. All the rayoutside <code>maxAngle</code> is evaluated to be zero, meaning no lightcan be emitted in that direction. The <code>FallOff</code> functioncalculates that attenuation giving the sampled outgoing light direction.I also implemented the <code>samplePhoton</code> function in order toapply spotlight in photon mapper. The sampled power is simply<code>I * falloff / pdf</code>, following implementation in mitsuba. Pdfof the sampled photon uses<code>Warp::squareToUniformSphereCapPdf(cosMaxAngle)</code> since thesampled ray can only exist in the cone area contrained by<code>maxAnlge</code>.</p><p>One problem is that since the spotlight is a delta emitter, theoriginal implementation of multiple importance sampling does not work onspotlight. So I added a flag <code>isDeltaEmitter</code> in<code>EmitterRecord</code>. Everytime a delta emitter is sampled, theweight of the emitter sampling should be 1 while weight of bsdf is 0. Imodified <code>direct_mis</code> and <code>path_mis</code> based on thisrule to fix the problem.</p><h3 id="validation-2">Validation</h3><p>To validate the correctness of the spotlight and the modification onthe integrator, I compared several rendered scenes in differentintegrator with mitsuba. The first scene is a simple floor withspotlight shooting from the above, rendered in direct integrator.</p><div class="twentytwenty-container" style="max-width:720px"><img src="floor-spotlight-nori.png" data-width="720"><img src="floor-spotlight-v06.png" data-width="720"></div><p>To validate the correctness in direct, path and photonmapperintegrator, I put a spotlight in the cbox scene. The noise level may beslightly different from mitsuba, but the overall brightness is correct.This can successfully validate the correctness of spotlight</p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_spotlight_direct_nori.png" data-width="600" data-height="600"><img src="cbox_spotlight_direct_v06.png" data-width="600" data-height="600"></div><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_spotlight_path_nori.png" data-width="600" data-height="600"><img src="cbox_spotlight_path_v06.png" data-width="600" data-height="600"></div><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_spotlight_pm_nori.png" data-width="600" data-height="600"><img src="cbox_spotlight_pm_v06.png" data-width="600" data-height="600"></div><h2 id="stratified-sampling">Stratified Sampling</h2><p><strong>Relevant Code</strong></p><ul><li><code>stratified.cpp</code></li><li><code>render.cpp</code></li></ul><h3 id="implementation-details-3">Implementation Details</h3><p><code>Stratified</code> class is derived from <code>Sampler</code>and implemented the stratified sampling. <code>prepare()</code> functionis used to initialize the sampler before start sampling. It is calledbefore the first sample starts. <code>generate()</code> function is usedto prepare for the sampling of the next image pixel.<code>advanced()</code> function is called every time before the nextsampling starts. <code>next1D</code> and <code>next2D</code> is used toretrieve the next 1d or 2d samples.</p><p>To enable sampling multiple dimensions, a <code>dimensionIndex</code>is maintained in the class. Similarly, <code>sampleIndex</code> and<code>pixelIndex</code> records the number of samples that are alreadysampled and the number of pixels sampled. When a sample is needed, firsta seed will be calculated based on a permutation base seed, dimensionindex and pixel index. The permutation sequences are thus the same forall the samples with the same <code>dimensionIndex</code> and<code>pixelIndex</code>, while different <code>dimensionIndex</code> and<code>pixelIndex</code> will have different sequences. Then, theposition of the current sampled region is retrieved with<code>sampleIndex</code>. Then a random offset is added according towhether the sampled is jittered or not.</p><p>In the <code>prepare()</code> function, I initialize the<code>sampleIndex</code> and the random number generators. A basepermutation seed is generated randomly. It can then be used to generatea permuation sequence of size <code>sampleCount</code>.<code>generate()</code> resets the dimension index and increment thepixelIndex. <code>advance</code> resets the pixel index and incrementthe sample index.</p><h3 id="validation-3">Validation</h3><p>In th warptest, I visualize the results of the stratified sampler andgrid sampler to show the difference between them. The result is shownbelow.</p><div class="twentytwenty-container" style="max-width:700px"><img src="stratified-sampler.png" data-width="700"><img src="grid-sampler.png" data-width="700"></div><p>I also rendered a simple scene which consists of a plane and a sphereto see the actual effects of the stratified sampler. The result is shownbelow. The stratified sampler can reduce the noise level in .</p><div class="twentytwenty-container" style="max-width:720px"><img src="sphere-independent-nori.png" data-width="720"><img src="sphere-stratified-nori.png" data-width="720"></div><h2id="disney-bsdf-metallic-specular-roughness-specular-tint-clearcoat">DisneyBSDF (metallic, specular, roughness, specular tint, clearcoat)</h2><p><strong>Relevant Code</strong></p><ul><li><code>disney.cpp</code></li><li><code>warp.h</code></li><li><code>warp.cpp</code></li><li><code>warptest.cpp</code></li></ul><h3 id="implementation-details-4">Implementation Details:</h3><p>The disney BSDF is implemented based on the paper <ahref="https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf">Physically-BasedShading at Disney</a>. The code basically refers to the implementationof <ahref="https://github.com/wdas/brdf/blob/main/src/brdfs/disney.brdf">DisneyBRDF explorer</a>.</p><p>There are ten parameters in Disney BRDF. Five of them, metallic,specular, roughness, specular tint and clearcoat graded. Disney BRDF isconsisted of three parts, which are diffuse, specular and clearcoat.Diffuse is influenced by metallic and roughness. Specular is influencedby specular, metallic, roughness and specular tint. Clearcoat isinfluenced by clearcoat. (Only graded ones are listed here) The specularand clearcoat are two microfacet models. The normal distribution ofspecular is GGX (GTR2), and the normal distribution of clearcoat isGTR1. The <code>squareToGTR2()</code>, <code>squareToGTR1()</code> andtheir pdf uses the formula derived in the appendix of the paper. The pdfare essentially the cosine weighted pdf of the normal distribution. The<code>eval()</code> function follows exactly the same as BRDF explorer.The choice of the sampling lobe refers to the implementation inmitsuba.</p><h3 id="validation-4">Validation</h3><p>The GTR1 and GTR2 are tested in the warptest. The results are shownbelow.</p><p><img src="warptest-GTR1-v.png" /> <img src="warptest-GTR1.png" /><img src="warptest-GTR2-v.png" /> <img src="warptest-GTR2.png" /></p><p>To validate the correctness of disney BSDF, I used the scene<code>cbox</code> for test. For each test, only the parameter beingtested is changed, with all other graded parameters fixed and all otherungraded parameters being 0. The results are shown below.</p><p><strong>Metallic=0 vs Metallic=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-m0.png" data-width="600"><img src="cbox_disney_nori-m1.png" data-width="600"></div><p><strong>Roughness=0 vs Roughness=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-r0.png" data-width="600"><img src="cbox_disney_nori-r1.png" data-width="600"></div><p><strong>Specular=0.1 vs Specular=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-s01.png" data-width="600"><img src="cbox_disney_nori-s1.png" data-width="600"></div><p><strong>Specular tint=0 vs Specular tint=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-st0.png" data-width="600"><img src="cbox_disney_nori-st1.png" data-width="600"></div><p><strong>Clearcoat=0 vs Clearcoat=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-c0.png" data-width="600"><img src="cbox_disney_nori-c1.png" data-width="600"></div><h2 id="progressive-photon-mapping">Progressive Photon Mapping</h2><p><strong>Relevant Code</strong></p><ul><li><code>progressivepm.cpp</code></li></ul><h3 id="implementation-details-5">Implementation Details</h3><p>The implementation of progressive photon mapping is basically basedon the paper <ahref="http://graphics.ucsd.edu/~henrik/papers/progressive_photon_mapping/progressive_photon_mapping.pdf">ProgressivePhoton Mapping</a>. The algorithm is consisted of two passes. In thefirst pass, we need to generate the visible hit points from the cameraand store them. In the second pass, we need to generate the photon maprepeated. After each photon map generation, we need to update the hitpoints with radius reduction and flux correction. A new rendered imagecan be generated right after eahc photon map generation process.</p><p>To make use of the current multiple thread rendering framework, theexecution process of the progressive photon mapper is a little differentfrom other integrators. In the <code>process()</code> function, weiterate all the image pixels and shoot <code>sampleCount</code> rays andperform path tracing for each ray and get the hit point on the diffusesurfaces. I define a function <code>persamplePreprocess()</code>. Thisfunction is called everytime a new sample starts. In this way, the<code>sampleCount</code> defined in the <code>sampler</code> is actuallythe number of photon map generation passes, while the actual number ofsamples for each pixel is the <code>sampleCount</code> defined in theprogressive photon mapper. Then in the<code>persamplePreprocess()</code> function, a new photon map isgenerated. Then, a new image is rendered in <code>Li()</code> function.Since now the pixel is needed to find the corresponding hit pointsstored, a new parameter <code>pixel</code> is added to the<code>Li()</code> function. First, the hit points information is updatedwith the new photon map. Then, the image is rendered with the updatedhit points. Doing these two steps at the same time in <code>Li()</code>function can make the best of the rendering framework to accelerate therendering process and is easy to implement even though it is a littleunintuitive.</p><h3 id="validation-5">Validation</h3><p>To validate the progressive photon mapper, I rendered cbox withprogressive photon mapper of 1 photon map generation pass and 1000photon map generation passes. I also used the same setting on mitsubafor comparison. Even though the results are not exactly the same, itproves that the progressive photon mapper is converging to the correctresults.</p><p><strong>My results</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_pmap_nori_mp1.png" data-width="600"><img src="cbox_pmap_nori_mp100.png" data-width="600"></div><p><strong>Mistuba results</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_pmap_v06_mp1.png" data-width="600"><img src="cbox_pmap_v06_mp100.png" data-width="600"></div>]]></content>
      
      
      <categories>
          
          <category> GameGraphics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graphics </tag>
            
            <tag> Project </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
