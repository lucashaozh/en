<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Geometry Processing Project</title>
      <link href="/en/2024/09/22/gamegraphics/geometryprocessingproject/"/>
      <url>/en/2024/09/22/gamegraphics/geometryprocessingproject/</url>
      
        <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#implicit-surface-reconstruction">Implicit SurfaceReconstruction</a></li><li><a href="#mesh-parameterization">Mesh Parameterization</a><ul><li><a href="#uniform-and-cotangent-laplacian">Uniform and cotangentLaplacian</a></li><li><a href="#lscm">LSCM</a></li><li><a href="#arap">ARAP</a></li></ul></li><li><a href="#shape-deformation">Shape Deformation</a></li><li><a href="#skinning-and-skeletal-animation">Skinning and SkeletalAnimation</a><ul><li><a href="#rotation-representation">Rotation Representation</a></li><li><a href="#harmonic-skinning-weights-on-selected-handles">Harmonicskinning weights on selected handles</a><ul><li><a href="#handle-selection">Handle selection</a></li><li><a href="#skinning-weights-visualization">Skinning weightsvisualization</a></li></ul></li><li><a href="#skeletal-animation">Skeletal animation</a></li><li><a href="#context-aware-per-vertex-lbs">Context-aware per-vertexLBS</a></li></ul></li></ul><!-- tocstop --><span id="more"></span><h2><span id="implicit-surfacereconstruction">Implicit SurfaceReconstruction</span></h2><ul><li>Compute an implicit MLS function approximating a 3D point cloud withgiven normals.</li><li>Sample the implicit function on a 3D volumetric grid.</li><li>Apply the marching cubes algorithm to extract a triangle mesh fromthe zero level set.</li></ul><p><img src="https://s2.loli.net/2024/09/21/zedTcXsnWpYtMxK.png" alt="Image 1" style="width: 45%; object-fit: contain;"></p><h2><span id="mesh-parameterization">Mesh Parameterization</span></h2><ul><li>Parameterize a mesh by minimizing four different distortion measureswith fixed or free boundaries.<ul><li>Spring energy (uniform Laplacian)</li><li>Dirichlet/harmonic energy (cotangent Laplacian)</li><li>Least Squares Conformal Maps (LSCM)</li><li>As-Rigid-As-Possible (ARAP)</li></ul></li><li>Visualize the distortion by color coding.</li></ul><h3><span id="uniform-and-cotangentlaplacian">Uniform and cotangentLaplacian</span></h3><p>Spring energy is defined as <span class="math display">\[\frac{1}{2}k_{i,j}\Vert \mathbf{u}_{i}-\mathbf{u}_{j} \Vert^{2}\]</span> Minimizing the spring enegy <span class="math display">\[\begin{align*}E(\mathbf{u}_{1}, \cdots, \mathbf{u}_{n})&amp;=\sum\limits\frac{1}{2}k_{i,j}\Vert \mathbf{u}_{i}-\mathbf{u}_{j} \Vert^{2}\\\frac{\partial{E(\mathbf{u}_{1},\cdots,\mathbf{u}_{n})}}{\partial\mathbf{u}_{i}}&amp;=\sum\limitsk_{i,j}(\mathbf{u}_{i}-\mathbf{u}_{j})=0 \\\sum\limits_{j \in \mathcal{N}(i)\cap \mathcal{B}}k_{ij}\mathbf{u}_{i}+\sum\limits_{j \in \mathcal{N}(i) \backslash\mathcal{B}} k_{i,j}(\mathbf{u}_{i}-\mathbf{u}_{j})&amp;= \sum\limits_{j\in \mathcal{N}(i)\cap \mathcal{B}} k_{i,j}\mathbf{u}_{j}\end{align*}\]</span></p><figure><img src="https://s2.loli.net/2024/04/25/7VlIFtjNv5zsaSR.png" alt="Solve linear system"><figcaption aria-hidden="true">Solve linear system</figcaption></figure><ul><li>Two choices of spring constants<ul><li>Uniform <span class="math inline">\(k_{i,j}=1\)</span></li><li>Cotan <span class="math inline">\(k_{i,j}=\cot{\phi_{i,j}}+\cot{\phi_{j,i}}\)</span></li></ul></li></ul><figure style="text-align: center"><img src="https://s2.loli.net/2024/09/22/tYRwjISQlyW2J1f.png" alt="Uniform Laplacian"><figcaption>Uniform Laplacian</figcaption></figure><figure style="text-align: center"><img src="https://s2.loli.net/2024/09/22/znycJjoqNCtbK19.png" alt="Cotan Laplacian"><figcaption>Cotan Laplacian</figcaption></figure><h3><span id="lscm">LSCM</span></h3>The LSCM distortion measure can be defined as <span class="math display">\[D(J) = \| J + J^T - (\text{tr}\, J) I \|_F^2\]</span><figure style="text-align: center"><img src="https://s2.loli.net/2024/09/22/Yyz2u4bKoF1q5Cw.png" alt="LSCM"><figcaption>Cotan Laplacian</figcaption></figure><h3><span id="arap">ARAP</span></h3><p>The ARAP distortion is defined as <span class="math display">\[D(J) = \| J - R\|_F^2\]</span> where <span class="math inline">\(R\)</span> is the cloestrotation matrix to <span class="math inline">\(J\)</span>. The ARAPprocedure follows the steps below: - Local step: The Jacobians for eachface of the current iterate are computed. Then for each Jacobian theclosest rotation matrix is found. This can be done using the SVD of<span class="math inline">\(J\)</span>. - Global step: Then for eachJacobian the closest rotation matrix is found, they are all assumed tobe fixed, and then minimize the ARAP distortion measure by solving alinear system.</p><figure style="text-align: center"><img src="https://s2.loli.net/2024/09/21/bQjHe6omEWdcXkS.png" alt="LSCM"><figcaption>ARAP</figcaption></figure><h2><span id="shape-deformation">Shape Deformation</span></h2><p>Implement multiresolution mesh editing algorithm to interactivelydeform 3D models. Construct a two-level multi-resolution surfacerepresentation and use naive Laplacian editing to deform it.</p><p><img src="https://s2.loli.net/2024/09/22/OXJNh1mUl3wf9v6.png"></p><p>Multiresolution mesh editing algorithm: 1. Remove high-frequencydetails by surface smoothing 2. Deform the smooth mesh 3. Transferhigh-frequency details back to the deformed surface</p><p><video src="videos/mesh_edit.mp4" controls><a href="videos/mesh_edit.mp4">Video</a></video></p><h2><span id="skinning-and-skeletalanimation">Skinning and SkeletalAnimation</span></h2><h3><span id="rotation-representation">Rotation Representation</span></h3><table><colgroup><col style="width: 7%"><col style="width: 30%"><col style="width: 30%"><col style="width: 30%"></colgroup><thead><tr><th style="text-align: center;">Representions</th><th style="text-align: center;">Short Description</th><th style="text-align: center;">pros</th><th style="text-align: center;">cons</th></tr></thead><tbody><tr><td style="text-align: center;">rotation matrix</td><td style="text-align: center;">3x3 Matrix to represent rotation in3D</td><td style="text-align: center;">Simple to use directly.</td><td style="text-align: center;">Requires 9 elements to compute. Hard tointerpolation. Direct interpolation leads to artifact.</td></tr><tr><td style="text-align: center;">euler angles</td><td style="text-align: center;">Use three angles (yaw, pitch, roll) torepresent 3D representation</td><td style="text-align: center;">Only three parameters needed torepresent rotations. Intuitive. Easy to transformed to rotationmatrix</td><td style="text-align: center;">The gimbal lock problem. Can result ininterpolation problems</td></tr><tr><td style="text-align: center;">axis angle</td><td style="text-align: center;">Rotation defined by an rotation axis andan angle</td><td style="text-align: center;">straightforward and intuitive, easily beconverted to and from a matrix</td><td style="text-align: center;">Angle choices is not unique. Cannotperform interpolation directly</td></tr><tr><td style="text-align: center;">quaternions</td><td style="text-align: center;">Use a four-tuple of real number(x,y,z,w)</td><td style="text-align: center;">very efficient for interpolation.Without gimbal lock. Only 4 parameters required.</td><td style="text-align: center;">Less intuitive. more complex totransformed to rotation matrix. Double cover problem</td></tr></tbody></table><h3><span id="harmonic-skinningweights-on-selected-handles">Harmonic skinningweights on selected handles</span></h3><h4><span id="handle-selection">Handle selection</span></h4><table><colgroup><col style="width: 5%"><col style="width: 30%"><col style="width: 32%"><col style="width: 32%"></colgroup><thead><tr><th style="text-align: center;">shape name</th><th style="text-align: center;">joint 1</th><th style="text-align: center;">joint 2</th><th style="text-align: center;">joint 3</th></tr></thead><tbody><tr><td style="text-align: center;">hand</td><td style="text-align: center;"><img align="center" src="res/handle0.png" width="300"></td><td style="text-align: center;"><img align="center" src="./res/handle10.png" width="300"></td><td style="text-align: center;"><img align="center" src="./res/handle12.png" width="300"></td></tr></tbody></table><h4><span id="skinning-weightsvisualization">Skinning weightsvisualization</span></h4><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th style="text-align: center;">shape name</th><th style="text-align: center;">joint 1</th><th style="text-align: center;">joint 2</th><th style="text-align: center;">joint 3</th></tr></thead><tbody><tr><td style="text-align: center;">hand</td><td style="text-align: center;"><img align="center" src="./res/weight0.png" width="300"></td><td style="text-align: center;"><img align="center" src="./res/weight10.png" width="300"></td><td style="text-align: center;"><img align="center" src="./res/weight12.png" width="300"></td></tr></tbody></table><h3><span id="skeletal-animation">Skeletal animation</span></h3><table><colgroup><col style="width: 32%"><col style="width: 32%"><col style="width: 34%"></colgroup><thead><tr><th style="text-align: center;">Linear Blend Skinning</th><th style="text-align: center;">Dual Quaternion Skinning</th><th style="text-align: center;">per-face + averaging quaternions</th></tr></thead><tbody><tr><td style="text-align: center;"><img align="center" src="./res/hand_lbs.gif" width="300"></td><td style="text-align: center;"><img align="center" src="./res/hand_dqs.gif" width="300"></td><td style="text-align: center;"><img align="center" src="./res/hand_perface.gif" width="300"></td></tr></tbody></table><h3><span id="context-aware-per-vertex-lbs">Context-aware per-vertex LBS</span></h3><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th style="text-align: center;">without context</th><th style="text-align: center;">with context</th></tr></thead><tbody><tr><td style="text-align: center;"><img align="center" src="./res/eg_lbs.gif" width="300"></td><td style="text-align: center;"><img align="center" src="./res/eg_contextaware.gif" width="300"></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> GameGraphics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graphics </tag>
            
            <tag> Project </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ray Tracing Renderer (Course Project)</title>
      <link href="/en/2023/12/24/gamegraphics/noriproject/"/>
      <url>/en/2023/12/24/gamegraphics/noriproject/</url>
      
        <content type="html"><![CDATA[<h1><span id="motivational-image">Motivational Image</span></h1><p>The scene that we want to render is a dimly lit, smoky bar, inspiredby the following two images. Rows of exquisite liquor bottles line theshelves, their labels veiled in the subtle play of light and shadow. Anashtray rests nearby, adorned with an unlit cigarette. A dim spotlightbathes the ashtray and the wine bottles. The scene is to capture theessence of the bar's mystique, inviting participants to let theirimagination run wild as they bring out the richness of the hiddenstories within the rendering. The following two images are ourmotivation images.</p><p><img src="https://s2.loli.net/2024/01/02/FHRUOCLqBpaJQgw.jpg"> <img src="https://s2.loli.net/2024/01/02/fexQVhryYm2gdnl.jpg"></p><h1><span id="final-images">Final Images</span></h1><p>Based on the motivation images, we rendered our final images.</p><p><img src="https://s2.loli.net/2024/01/02/SVn3gKIpMBth4ND.png"></p><span id="more"></span><h1><span id="features-implemented">Features implemented</span></h1><h2><span id="images-as-texture">Images as Texture</span></h2><p><strong>Relevant Code</strong></p><ul><li><code>imagetexture.cpp</code></li></ul><h3><span id="implementation-details">Implementation Details</span></h3><p>To load the texture from external files, I used<code>stb_image</code> to read the image data. I defined a<code>ImageData</code> class similar to <code>Bitmap</code> class, whichis drived from <code>Eigen::Array</code> to store the image data. Theread image data is probably in sRGB color space. Therefore, we providean option boolean field <code>raw</code> in the <code>img_texture</code>property to indicate whether use raw image color space or convert it tolinear color space with <code>toLinearRGB()</code> function. We alsoprovide two difference filter modes, <code>nearest</code> and<code>bilinear</code>.</p><h3><span id="validation">Validation</span></h3><p>To validate the results, we used a texture from the website <a href="https://www.artstation.com/blogs/zeeshannasir/peYz/reference-uv-map-grids">Reference:UV Map Grids</a> and the scenes in assignment for rendering. The resultslook right. We also compared our results with mitsuba. Since thecoordinate used in mitsuba is right-handed while nori's is left-handed,the up axis coordinate is inversed so that the scene representation isthe same. The results are quite similar, while the results of mitsubaseem to be more vivid than ours. I think this may due to the usage ofmipmap in mitsuba. The overall results are correct.</p><div class="twentytwenty-container" style="max-width:720px"><img src="plane-texture-nori.png" data-width="720"><img src="plane-texture-v06.png" data-width="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="floor-texture-nori-nearest.png" data-width="720"><img src="floor-texture-v06-nearest.png" data-width="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="mesh-texture-nori-bilinear.png" data-width="720"><img src="mesh-texture-v06-bilinear.png" data-width="720"></div><h2><span id="normal-mapping">Normal Mapping</span></h2><p><strong>Relevant Code</strong></p><ul><li><code>mesh.h</code></li><li><code>mesh.cpp</code></li><li><code>shape.h</code></li><li><code>shape.cpp</code></li></ul><h3><span id="implementation-details">Implementation Details</span></h3><p>Normal mapping requires us to update the mesh normal with the normalsprovided in the normal textures. Since the frame normal is calculated inthe <code>setHitInformation()</code> function in <code>Shape</code>, Idecided to read the normal map and update the normals here. I added afield of type <code>Texture&lt;Color3f&gt; m_normalMap</code>. Thenormal map can be loaded with the Image as Texture implemented before.After reading the texture evaled value, it has to be converted to range[-1, 1]. Currently, only <code>Mesh</code> supports computing shaingframe with normal mapping. In the <code>setHitInformation()</code>function in <code>Mesh</code>, the local normal read from the normalmapping needs to be converted to the world normal using the local framefirst. However, if the default frame setup is used, the result isdiscontinuous and is absolutely wrong. Therefore, I need to constructthe right local frame first. I followed the formula in <a href="https://learnopengl.com/Advanced-Lighting/Normal-Mapping">learnopengl</a>.</p><p><span class="math display">\[\begin{bmatrix}T_{x}&amp;T_{y}&amp;T_{z}\\B_{x}&amp;B_{y}&amp;B_{z}\end{bmatrix}=\frac{1}{\Delta U_{1}\DeltaV_{2}-\Delta U_{2}\Delta V_{1}}\begin{bmatrix}\Delta V_{2}&amp;-\DeltaV_{1} \\ -\Delta U_{2} &amp; \Delta U_{1}\end{bmatrix}\begin{bmatrix}E_{1x}&amp;E_{1y}&amp;E_{1z}\\E_{2x}&amp;E_{2y}&amp;E_{2z}\end{bmatrix}\]</span></p><p>where <span class="math inline">\(E_{1}\)</span> and <span class="math inline">\(E_{2}\)</span> are two edges of the triangle, and<span class="math inline">\(\Delta U, \Delta V\)</span> are thecorresponding uv offsets of the edges. <span class="math inline">\(T\)</span> and <span class="math inline">\(B\)</span> is <code>dpdv</code> and<code>dpdu</code>. Then, we can construct the local frame correctly with<code>dpdv</code> and <code>dpdu</code>.</p><h3><span id="validation">Validation</span></h3><p>To validate the results, the results of nori are compared to theresults of mitsuba.</p><div class="twentytwenty-container" style="max-width:720px"><img src="plane-normalmap-c-nori.png" data-width="720" data-height="720"><img src="plane-normalmap-c-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="plane-normalmap-t-nori.png" data-width="720" data-height="720"><img src="plane-normalmap-t-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="floor-normalmap-c-nori.png" data-width="720" data-height="720"><img src="floor-normalmap-c-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="floor-normalmap-t-nori.png" data-width="720" data-height="720"><img src="floor-normalmap-t-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="sphere-mesh-normalmap-c-nori.png" data-width="720" data-height="720"><img src="sphere-mesh-normalmap-c-v06.png" data-width="720" data-height="720"></div><div class="twentytwenty-container" style="max-width:720px"><img src="sphere-mesh-normalmap-t-nori.png" data-width="720" data-height="720"><img src="sphere-mesh-normalmap-t-v06.png" data-width="720" data-height="720"></div><h2><span id="emitter-spotlight-5pt">Emitter (Spotlight) (5pt)</span></h2><p><strong>Relevant Code</strong></p><ul><li><code>spotlight.cpp</code></li><li><code>path_mis.cpp</code></li><li><code>direct_mis.cpp</code></li></ul><h3><span id="implementation-details">Implementation Details</span></h3><p>The implementation of the spotlight follows the mistuba designs. Ithas three properties, <code>intensity</code>, <code>maxAngle</code> and<code>beamAngle</code>. <code>intensity</code> is the max irridiance ofemitter at the center. All the ray emitted within the beamAngle has anirridiance of <code>intensity</code>. The emitted intensity of the raybegins to attenuate linearly between beamAngle and maxAngle. All the rayoutside <code>maxAngle</code> is evaluated to be zero, meaning no lightcan be emitted in that direction. The <code>FallOff</code> functioncalculates that attenuation giving the sampled outgoing light direction.I also implemented the <code>samplePhoton</code> function in order toapply spotlight in photon mapper. The sampled power is simply<code>I * falloff / pdf</code>, following implementation in mitsuba. Pdfof the sampled photon uses<code>Warp::squareToUniformSphereCapPdf(cosMaxAngle)</code> since thesampled ray can only exist in the cone area contrained by<code>maxAnlge</code>.</p><p>One problem is that since the spotlight is a delta emitter, theoriginal implementation of multiple importance sampling does not work onspotlight. So I added a flag <code>isDeltaEmitter</code> in<code>EmitterRecord</code>. Everytime a delta emitter is sampled, theweight of the emitter sampling should be 1 while weight of bsdf is 0. Imodified <code>direct_mis</code> and <code>path_mis</code> based on thisrule to fix the problem.</p><h3><span id="validation">Validation</span></h3><p>To validate the correctness of the spotlight and the modification onthe integrator, I compared several rendered scenes in differentintegrator with mitsuba. The first scene is a simple floor withspotlight shooting from the above, rendered in direct integrator.</p><div class="twentytwenty-container" style="max-width:720px"><img src="floor-spotlight-nori.png" data-width="720"><img src="floor-spotlight-v06.png" data-width="720"></div><p>To validate the correctness in direct, path and photonmapperintegrator, I put a spotlight in the cbox scene. The noise level may beslightly different from mitsuba, but the overall brightness is correct.This can successfully validate the correctness of spotlight</p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_spotlight_direct_nori.png" data-width="600" data-height="600"><img src="cbox_spotlight_direct_v06.png" data-width="600" data-height="600"></div><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_spotlight_path_nori.png" data-width="600" data-height="600"><img src="cbox_spotlight_path_v06.png" data-width="600" data-height="600"></div><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_spotlight_pm_nori.png" data-width="600" data-height="600"><img src="cbox_spotlight_pm_v06.png" data-width="600" data-height="600"></div><h2><span id="stratified-sampling">Stratified Sampling</span></h2><p><strong>Relevant Code</strong></p><ul><li><code>stratified.cpp</code></li><li><code>render.cpp</code></li></ul><h3><span id="implementation-details">Implementation Details</span></h3><p><code>Stratified</code> class is derived from <code>Sampler</code>and implemented the stratified sampling. <code>prepare()</code> functionis used to initialize the sampler before start sampling. It is calledbefore the first sample starts. <code>generate()</code> function is usedto prepare for the sampling of the next image pixel.<code>advanced()</code> function is called every time before the nextsampling starts. <code>next1D</code> and <code>next2D</code> is used toretrieve the next 1d or 2d samples.</p><p>To enable sampling multiple dimensions, a <code>dimensionIndex</code>is maintained in the class. Similarly, <code>sampleIndex</code> and<code>pixelIndex</code> records the number of samples that are alreadysampled and the number of pixels sampled. When a sample is needed, firsta seed will be calculated based on a permutation base seed, dimensionindex and pixel index. The permutation sequences are thus the same forall the samples with the same <code>dimensionIndex</code> and<code>pixelIndex</code>, while different <code>dimensionIndex</code> and<code>pixelIndex</code> will have different sequences. Then, theposition of the current sampled region is retrieved with<code>sampleIndex</code>. Then a random offset is added according towhether the sampled is jittered or not.</p><p>In the <code>prepare()</code> function, I initialize the<code>sampleIndex</code> and the random number generators. A basepermutation seed is generated randomly. It can then be used to generatea permuation sequence of size <code>sampleCount</code>.<code>generate()</code> resets the dimension index and increment thepixelIndex. <code>advance</code> resets the pixel index and incrementthe sample index.</p><h3><span id="validation">Validation</span></h3><p>In th warptest, I visualize the results of the stratified sampler andgrid sampler to show the difference between them. The result is shownbelow.</p><div class="twentytwenty-container" style="max-width:700px"><img src="stratified-sampler.png" data-width="700"><img src="grid-sampler.png" data-width="700"></div><p>I also rendered a simple scene which consists of a plane and a sphereto see the actual effects of the stratified sampler. The result is shownbelow. The stratified sampler can reduce the noise level in .</p><div class="twentytwenty-container" style="max-width:720px"><img src="sphere-independent-nori.png" data-width="720"><img src="sphere-stratified-nori.png" data-width="720"></div><h2><span id="disneybsdf-metallic-specular-roughness-specular-tint-clearcoat">DisneyBSDF (metallic, specular, roughness, specular tint, clearcoat)</span></h2><p><strong>Relevant Code</strong></p><ul><li><code>disney.cpp</code></li><li><code>warp.h</code></li><li><code>warp.cpp</code></li><li><code>warptest.cpp</code></li></ul><h3><span id="implementation-details">Implementation Details:</span></h3><p>The disney BSDF is implemented based on the paper <a href="https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf">Physically-BasedShading at Disney</a>. The code basically refers to the implementationof <a href="https://github.com/wdas/brdf/blob/main/src/brdfs/disney.brdf">DisneyBRDF explorer</a>.</p><p>There are ten parameters in Disney BRDF. Five of them, metallic,specular, roughness, specular tint and clearcoat graded. Disney BRDF isconsisted of three parts, which are diffuse, specular and clearcoat.Diffuse is influenced by metallic and roughness. Specular is influencedby specular, metallic, roughness and specular tint. Clearcoat isinfluenced by clearcoat. (Only graded ones are listed here) The specularand clearcoat are two microfacet models. The normal distribution ofspecular is GGX (GTR2), and the normal distribution of clearcoat isGTR1. The <code>squareToGTR2()</code>, <code>squareToGTR1()</code> andtheir pdf uses the formula derived in the appendix of the paper. The pdfare essentially the cosine weighted pdf of the normal distribution. The<code>eval()</code> function follows exactly the same as BRDF explorer.The choice of the sampling lobe refers to the implementation inmitsuba.</p><h3><span id="validation">Validation</span></h3><p>The GTR1 and GTR2 are tested in the warptest. The results are shownbelow.</p><p><img src="warptest-GTR1-v.png"> <img src="warptest-GTR1.png"><img src="warptest-GTR2-v.png"> <img src="warptest-GTR2.png"></p><p>To validate the correctness of disney BSDF, I used the scene<code>cbox</code> for test. For each test, only the parameter beingtested is changed, with all other graded parameters fixed and all otherungraded parameters being 0. The results are shown below.</p><p><strong>Metallic=0 vs Metallic=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-m0.png" data-width="600"><img src="cbox_disney_nori-m1.png" data-width="600"></div><p><strong>Roughness=0 vs Roughness=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-r0.png" data-width="600"><img src="cbox_disney_nori-r1.png" data-width="600"></div><p><strong>Specular=0.1 vs Specular=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-s01.png" data-width="600"><img src="cbox_disney_nori-s1.png" data-width="600"></div><p><strong>Specular tint=0 vs Specular tint=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-st0.png" data-width="600"><img src="cbox_disney_nori-st1.png" data-width="600"></div><p><strong>Clearcoat=0 vs Clearcoat=1</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_disney_nori-c0.png" data-width="600"><img src="cbox_disney_nori-c1.png" data-width="600"></div><h2><span id="progressive-photon-mapping">Progressive Photon Mapping</span></h2><p><strong>Relevant Code</strong></p><ul><li><code>progressivepm.cpp</code></li></ul><h3><span id="implementation-details">Implementation Details</span></h3><p>The implementation of progressive photon mapping is basically basedon the paper <a href="http://graphics.ucsd.edu/~henrik/papers/progressive_photon_mapping/progressive_photon_mapping.pdf">ProgressivePhoton Mapping</a>. The algorithm is consisted of two passes. In thefirst pass, we need to generate the visible hit points from the cameraand store them. In the second pass, we need to generate the photon maprepeated. After each photon map generation, we need to update the hitpoints with radius reduction and flux correction. A new rendered imagecan be generated right after eahc photon map generation process.</p><p>To make use of the current multiple thread rendering framework, theexecution process of the progressive photon mapper is a little differentfrom other integrators. In the <code>process()</code> function, weiterate all the image pixels and shoot <code>sampleCount</code> rays andperform path tracing for each ray and get the hit point on the diffusesurfaces. I define a function <code>persamplePreprocess()</code>. Thisfunction is called everytime a new sample starts. In this way, the<code>sampleCount</code> defined in the <code>sampler</code> is actuallythe number of photon map generation passes, while the actual number ofsamples for each pixel is the <code>sampleCount</code> defined in theprogressive photon mapper. Then in the<code>persamplePreprocess()</code> function, a new photon map isgenerated. Then, a new image is rendered in <code>Li()</code> function.Since now the pixel is needed to find the corresponding hit pointsstored, a new parameter <code>pixel</code> is added to the<code>Li()</code> function. First, the hit points information is updatedwith the new photon map. Then, the image is rendered with the updatedhit points. Doing these two steps at the same time in <code>Li()</code>function can make the best of the rendering framework to accelerate therendering process and is easy to implement even though it is a littleunintuitive.</p><h3><span id="validation">Validation</span></h3><p>To validate the progressive photon mapper, I rendered cbox withprogressive photon mapper of 1 photon map generation pass and 1000photon map generation passes. I also used the same setting on mitsubafor comparison. Even though the results are not exactly the same, itproves that the progressive photon mapper is converging to the correctresults.</p><p><strong>My results</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_pmap_nori_mp1.png" data-width="600"><img src="cbox_pmap_nori_mp100.png" data-width="600"></div><p><strong>Mistuba results</strong></p><div class="twentytwenty-container" style="max-width:600px"><img src="cbox_pmap_v06_mp1.png" data-width="600"><img src="cbox_pmap_v06_mp100.png" data-width="600"></div>]]></content>
      
      
      <categories>
          
          <category> GameGraphics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graphics </tag>
            
            <tag> Project </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
